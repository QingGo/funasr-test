import argparse
import logging
import signal
import time
from dataclasses import dataclass

import numpy as np
import pyaudio
from funasr import AutoModel

# å±è”½ç¹æ‚æ—¥å¿—
logging.getLogger("modelscope").setLevel(logging.ERROR)
logging.getLogger("funasr").setLevel(logging.ERROR)


@dataclass(frozen=True)
class AudioConfig:
    sample_rate: int = 16000
    chunk: int = 960  # 60ms
    window_size: int = 4800  # 300ms
    step_size: int = 960  # 60ms
    format: int = pyaudio.paInt16
    channels: int = 1


@dataclass(frozen=True)
class StreamConfig:
    chunk_size: tuple = (0, 10, 5)
    encoder_chunk_look_back: int = 2
    decoder_chunk_look_back: int = 1


@dataclass
class RuntimeState:
    is_running: bool = True
    all_audio_data: list = None

    def __post_init__(self):
        if self.all_audio_data is None:
            self.all_audio_data = []


def signal_handler(sig, frame, state: RuntimeState):
    print("\næ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
    state.is_running = False


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="FunASR å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
    parser.add_argument("--audio_file", type=str, help="å½•éŸ³æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæµå¼é‡æ”¾æµ‹è¯•")
    parser.add_argument("--benchmark", type=str, help="åŸºå‡†æ–‡æœ¬ï¼Œç”¨äºè‡ªåŠ¨éªŒè¯")
    parser.add_argument("--mic", action="store_true", help="ä½¿ç”¨éº¦å…‹é£å®æ—¶è¾“å…¥")
    parser.add_argument("--gain", type=float, default=3.0, help="éŸ³é¢‘å¢ç›Šè°ƒæ•´ï¼Œé»˜è®¤3.0")
    parser.add_argument(
        "--threshold",
        type=float,
        default=100.0,
        help="éŸ³é‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è§†ä¸ºé™éŸ³ï¼Œé»˜è®¤100.0",
    )
    return parser


def print_banner(audio_cfg: AudioConfig, gain: float, volume_threshold: float) -> None:
    print("=== FunASR å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ ===")
    print("æ”¯æŒå®æ—¶éº¦å…‹é£è¾“å…¥ã€æ»‘åŠ¨çª—å£ã€å½•éŸ³ä¿å­˜å’Œè‡ªåŠ¨éªŒè¯åŠŸèƒ½\n")
    print("ğŸ›ï¸  é…ç½®å‚æ•°:")
    print(
        f"   CHUNK: {audio_cfg.chunk} ({audio_cfg.chunk / audio_cfg.sample_rate * 1000:.0f}ms)"
    )
    print(
        f"   æ»‘åŠ¨çª—å£: {audio_cfg.window_size} "
        f"({audio_cfg.window_size / audio_cfg.sample_rate * 1000:.0f}ms)"
    )
    print(
        f"   æ»‘åŠ¨æ­¥é•¿: {audio_cfg.step_size} "
        f"({audio_cfg.step_size / audio_cfg.sample_rate * 1000:.0f}ms)"
    )
    print(f"   é‡‡æ ·ç‡: {audio_cfg.sample_rate}Hz")
    print(f"   éŸ³é¢‘å¢ç›Š: {gain}")
    print(f"   éŸ³é‡é˜ˆå€¼: {volume_threshold}")
    print()


def load_model() -> AutoModel:
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    model = AutoModel(
        model="paraformer-zh-streaming",
        model_revision="v2.0.4",
        disable_update=True,
        verbose=False,
    )
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")
    return model


def preprocess_audio(audio_chunk: np.ndarray, gain: float) -> np.ndarray:
    processed = audio_chunk.astype(np.float32) * gain
    processed = np.clip(processed, -32768, 32767)
    return processed.astype(np.int16)


def save_recording(audio_data: np.ndarray) -> str:
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"recording_{timestamp}.npy"
    np.save(filename, audio_data)
    print(f"\nå½•éŸ³å·²ä¿å­˜ä¸º: {filename}")
    return filename


def calculate_similarity(text1: str, text2: str) -> float:
    """ä½¿ç”¨é›†åˆç›¸ä¼¼åº¦è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
    if not text1 or not text2:
        return 0.0
    set1 = set(text1)
    set2 = set(text2)
    common = set1.intersection(set2)
    if not set2:
        return 0.0
    return len(common) / len(set2)


def merge_stream_text(current_text: str, new_text: str) -> str:
    if not new_text:
        return current_text
    if not current_text:
        return new_text
    if new_text in current_text:
        return current_text

    # å¤„ç†æµå¼è¾“å‡ºçš„å‰åé‡å ï¼Œé¿å…é‡å¤å­—
    max_overlap = 0
    max_len = min(len(current_text), len(new_text))
    for i in range(1, max_len + 1):
        if current_text[-i:] == new_text[:i]:
            max_overlap = i
    return current_text + new_text[max_overlap:]


def stream_recognition_from_samples(
    sample_iter,
    model: AutoModel,
    audio_cfg: AudioConfig,
    stream_cfg: StreamConfig,
    state: RuntimeState,
    gain: float,
    volume_threshold: float,
    label: str = "",
) -> str:
    cache = {}
    full_text = ""
    audio_cache = []
    # stream_cfg.chunk_size[1] ä»¥ 60ms ä¸ºå•ä½ï¼Œ10 -> 600ms
    chunk_stride_samples = int(stream_cfg.chunk_size[1] * 960)
    max_buffer = max(audio_cfg.window_size, chunk_stride_samples)

    if label:
        print(label)

    for audio_chunk in sample_iter:
        if not state.is_running:
            break

        # è®°å½•å…¨éƒ¨éŸ³é¢‘ç”¨äºæœ€ç»ˆè¯†åˆ«/å›æ”¾
        state.all_audio_data.extend(audio_chunk)

        # ç´¯ç§¯ç¼“å­˜ï¼Œç›´åˆ°è¾¾åˆ°æ¨¡å‹éœ€è¦çš„æ­¥é•¿
        audio_cache.extend(audio_chunk)
        if len(audio_cache) > max_buffer:
            audio_cache = audio_cache[-max_buffer:]

        current_volume = np.abs(audio_chunk).mean()
        if current_volume < volume_threshold:
            volume_status = "ğŸ”‡ é™éŸ³"
        elif current_volume < volume_threshold * 2:
            volume_status = "ğŸ”‰ å°å£°"
        elif current_volume < volume_threshold * 5:
            volume_status = "ğŸ”Š æ­£å¸¸"
        else:
            volume_status = "ğŸ”ŠğŸ”Š å¤§å£°"

        if len(audio_cache) < chunk_stride_samples:
            debug_info = (
                f"\r{volume_status} | éŸ³é‡: {current_volume:5.1f} | "
                f"ç¼“å­˜: {len(audio_cache):4d} | è¯†åˆ«ç»“æœ: {full_text}"
            )
            print(debug_info, end="", flush=True)
            continue

        processed_audio = preprocess_audio(
            np.array(audio_cache[:chunk_stride_samples]),
            gain=gain,
        )
        audio_cache = audio_cache[chunk_stride_samples:]

        recognize_start = time.time()
        res = model.generate(
            input=processed_audio,
            cache=cache,
            is_final=False,
            chunk_size=list(stream_cfg.chunk_size),
            encoder_chunk_look_back=stream_cfg.encoder_chunk_look_back,
            decoder_chunk_look_back=stream_cfg.decoder_chunk_look_back,
            disable_pbar=True,
            disable_log=True,
        )
        recognize_delay = (time.time() - recognize_start) * 1000

        partial_text = res[0]["text"] if (res and res[0]["text"]) else ""
        if partial_text:
            full_text = merge_stream_text(full_text, partial_text)

        debug_info = (
            f"\r{volume_status} | éŸ³é‡: {current_volume:5.1f} | "
            f"å»¶è¿Ÿ: {recognize_delay:4.1f}ms | è¯†åˆ«ç»“æœ: {full_text}"
        )
        print(debug_info, end="", flush=True)

    # flush remaining cache
    if audio_cache:
        processed_audio = preprocess_audio(np.array(audio_cache), gain=gain)
        res = model.generate(
            input=processed_audio,
            cache=cache,
            is_final=True,
            chunk_size=list(stream_cfg.chunk_size),
            encoder_chunk_look_back=stream_cfg.encoder_chunk_look_back,
            decoder_chunk_look_back=stream_cfg.decoder_chunk_look_back,
            disable_pbar=True,
            disable_log=True,
        )
    else:
        res = model.generate(
            input=np.array([], dtype=np.int16),
            cache=cache,
            is_final=True,
            chunk_size=list(stream_cfg.chunk_size),
            encoder_chunk_look_back=stream_cfg.encoder_chunk_look_back,
            decoder_chunk_look_back=stream_cfg.decoder_chunk_look_back,
            disable_pbar=True,
            disable_log=True,
        )

    if res and res[0]["text"]:
        full_text = merge_stream_text(full_text, res[0]["text"])
        print(f"\nğŸ“ æœ€ç»ˆæµå¼è¯†åˆ«ç»“æœ: {full_text}")

    return full_text


def final_full_recognition(
    model: AutoModel,
    stream_cfg: StreamConfig,
    audio_data: list,
    gain: float,
) -> str:
    if not audio_data:
        return ""

    print("\n\nğŸ” ä½¿ç”¨å®Œæ•´å½•éŸ³è¿›è¡Œæœ€ç»ˆè¯†åˆ«...")
    full_audio = np.array(audio_data)
    processed_full = preprocess_audio(full_audio, gain=gain)

    res = model.generate(
        input=processed_full,
        cache={},
        is_final=True,
        chunk_size=list(stream_cfg.chunk_size),
        encoder_chunk_look_back=stream_cfg.encoder_chunk_look_back,
        decoder_chunk_look_back=stream_cfg.decoder_chunk_look_back,
        disable_pbar=True,
        disable_log=True,
    )

    if res and res[0]["text"]:
        final_text = res[0]["text"]
        print(f"ğŸ“ å®Œæ•´å½•éŸ³è¯†åˆ«ç»“æœ: {final_text}")
        return final_text
    return ""


def real_time_recognition(
    model: AutoModel,
    audio_cfg: AudioConfig,
    stream_cfg: StreamConfig,
    state: RuntimeState,
    gain: float,
    volume_threshold: float,
) -> str:
    p = pyaudio.PyAudio()
    stream = p.open(
        format=audio_cfg.format,
        channels=audio_cfg.channels,
        rate=audio_cfg.sample_rate,
        input=True,
        frames_per_buffer=audio_cfg.chunk,
    )

    print("ğŸ¤ å¼€å§‹å®æ—¶å½•éŸ³å’Œè¯†åˆ«...")
    print("   æŒ‰ Ctrl+C åœæ­¢\n")

    def mic_iter():
        while state.is_running:
            data = stream.read(audio_cfg.chunk, exception_on_overflow=False)
            yield np.frombuffer(data, dtype=np.int16)

    try:
        result = stream_recognition_from_samples(
            mic_iter(),
            model=model,
            audio_cfg=audio_cfg,
            stream_cfg=stream_cfg,
            state=state,
            gain=gain,
            volume_threshold=volume_threshold,
        )
    except Exception as e:
        print(f"\nå½•éŸ³å‡ºé”™: {e}")
        result = ""
    finally:
        # æ¸…ç†èµ„æº
        stream.stop_stream()
        stream.close()
        p.terminate()

        # ä¿å­˜å½•éŸ³
        if state.all_audio_data:
            save_recording(np.array(state.all_audio_data))

    return result


def file_streaming_recognition(
    audio_file: str,
    model: AutoModel,
    audio_cfg: AudioConfig,
    stream_cfg: StreamConfig,
    state: RuntimeState,
    gain: float,
    volume_threshold: float,
) -> str:
    print(f"ğŸ“ ä½¿ç”¨å½•éŸ³æ–‡ä»¶è¿›è¡Œæµ‹è¯•: {audio_file}")
    audio_data = np.load(audio_file)
    state.all_audio_data = audio_data.tolist()

    print(f"éŸ³é¢‘æ—¶é•¿: {len(audio_data) / audio_cfg.sample_rate:.2f}ç§’")
    print(f"å¹³å‡éŸ³é‡: {np.abs(audio_data).mean():.2f}")

    def file_iter():
        for i in range(0, len(audio_data), audio_cfg.chunk):
            yield audio_data[i : i + audio_cfg.chunk]

    final_text = stream_recognition_from_samples(
        file_iter(),
        model=model,
        audio_cfg=audio_cfg,
        stream_cfg=stream_cfg,
        state=state,
        gain=gain,
        volume_threshold=volume_threshold,
        label="\nå¼€å§‹æµå¼è¯†åˆ«...",
    )
    print(f"\næœ€ç»ˆè¯†åˆ«ç»“æœ: {final_text}")
    return final_text


def run_benchmark(result: str, benchmark: str) -> None:
    if not benchmark or not result:
        return

    print("\nâœ… è‡ªåŠ¨éªŒè¯:")
    print(f"   åŸºå‡†æ–‡æœ¬: {benchmark}")
    print(f"   è¯†åˆ«ç»“æœ: {result}")
    similarity = calculate_similarity(result, benchmark)
    print(f"   ç›¸ä¼¼åº¦: {similarity:.2f}")
    if similarity >= 0.7:
        print("   éªŒè¯çŠ¶æ€: é€šè¿‡ âœ…")
    else:
        print("   éªŒè¯çŠ¶æ€: æœªé€šè¿‡ âŒ")


def main() -> None:
    args = build_arg_parser().parse_args()

    audio_cfg = AudioConfig()
    stream_cfg = StreamConfig()
    state = RuntimeState()

    signal.signal(signal.SIGINT, lambda sig, frame: signal_handler(sig, frame, state))

    print_banner(audio_cfg, gain=args.gain, volume_threshold=args.threshold)
    model = load_model()

    if args.audio_file:
        result = file_streaming_recognition(
            args.audio_file,
            model=model,
            audio_cfg=audio_cfg,
            stream_cfg=stream_cfg,
            state=state,
            gain=args.gain,
            volume_threshold=args.threshold,
        )
    elif args.mic:
        result = real_time_recognition(
            model=model,
            audio_cfg=audio_cfg,
            stream_cfg=stream_cfg,
            state=state,
            gain=args.gain,
            volume_threshold=args.threshold,
        )
        final_text = final_full_recognition(
            model=model,
            stream_cfg=stream_cfg,
            audio_data=state.all_audio_data,
            gain=args.gain,
        )
        if final_text:
            print(f"ğŸ”„ å®æ—¶è¯†åˆ«ç»“æœ: {result}")
            result = final_text
    else:
        print("è¯·æŒ‡å®š --mic ä½¿ç”¨éº¦å…‹é£ï¼Œæˆ– --audio_file æŒ‡å®šéŸ³é¢‘æ–‡ä»¶")
        return

    run_benchmark(result, args.benchmark)


if __name__ == "__main__":
    main()
    print("\n=== ç¨‹åºç»“æŸ ===")

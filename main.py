import pyaudio
import numpy as np
from funasr import AutoModel
import logging
import time
import argparse
import signal
import os

# å±è”½ç¹æ‚æ—¥å¿—
logging.getLogger("modelscope").setLevel(logging.ERROR)
logging.getLogger("funasr").setLevel(logging.ERROR)

# å…¨å±€å˜é‡
is_running = True
cleanup_called = False
all_audio_data = []

# ä¿¡å·å¤„ç†
def signal_handler(sig, frame):
    global is_running
    print("\næ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
    is_running = False

# å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description="FunASR å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
parser.add_argument("--audio_file", type=str, help="å½•éŸ³æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæµå¼é‡æ”¾æµ‹è¯•")
parser.add_argument("--benchmark", type=str, help="åŸºå‡†æ–‡æœ¬ï¼Œç”¨äºè‡ªåŠ¨éªŒè¯")
parser.add_argument("--mic", action="store_true", help="ä½¿ç”¨éº¦å…‹é£å®æ—¶è¾“å…¥")
parser.add_argument("--gain", type=float, default=3.0, help="éŸ³é¢‘å¢ç›Šè°ƒæ•´ï¼Œé»˜è®¤3.0")
parser.add_argument("--threshold", type=float, default=100.0, help="éŸ³é‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è§†ä¸ºé™éŸ³ï¼Œé»˜è®¤100.0")
args = parser.parse_args()

print("=== FunASR å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ ===")
print("æ”¯æŒå®æ—¶éº¦å…‹é£è¾“å…¥ã€æ»‘åŠ¨çª—å£ã€å½•éŸ³ä¿å­˜å’Œè‡ªåŠ¨éªŒè¯åŠŸèƒ½\n")

# éŸ³é¢‘é…ç½®
SAMPLE_RATE = 16000
CHUNK = 960  # 60msï¼Œæ¯æ¬¡è¯»å–çš„éŸ³é¢‘å—å¤§å°
WINDOW_SIZE = 4800  # 300msï¼Œæ»‘åŠ¨çª—å£å¤§å°
STEP_SIZE = 960     # 60msï¼Œæ»‘åŠ¨æ­¥é•¿
FORMAT = pyaudio.paInt16
CHANNELS = 1
gain = args.gain
volume_threshold = args.threshold

print(f"ğŸ›ï¸  é…ç½®å‚æ•°:")
print(f"   CHUNK: {CHUNK} ({CHUNK/SAMPLE_RATE*1000:.0f}ms)")
print(f"   æ»‘åŠ¨çª—å£: {WINDOW_SIZE} ({WINDOW_SIZE/SAMPLE_RATE*1000:.0f}ms)")
print(f"   æ»‘åŠ¨æ­¥é•¿: {STEP_SIZE} ({STEP_SIZE/SAMPLE_RATE*1000:.0f}ms)")
print(f"   é‡‡æ ·ç‡: {SAMPLE_RATE}Hz")
print(f"   éŸ³é¢‘å¢ç›Š: {gain}")
print(f"   éŸ³é‡é˜ˆå€¼: {volume_threshold}")
print()

# åŠ è½½æ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = AutoModel(
    model="paraformer-zh-streaming", 
    model_revision="v2.0.4",
    disable_update=True,
    verbose=False
)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

# éŸ³é¢‘é¢„å¤„ç†
def preprocess_audio(audio_chunk, gain=3.0):
    processed = audio_chunk.astype(np.float32) * gain
    processed = np.clip(processed, -32768, 32767)
    return processed.astype(np.int16)

# ä¿å­˜å½•éŸ³
def save_recording(audio_data):
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"recording_{timestamp}.npy"
    np.save(filename, audio_data)
    print(f"\nå½•éŸ³å·²ä¿å­˜ä¸º: {filename}")
    return filename

# ç›¸ä¼¼åº¦è®¡ç®—
def calculate_similarity(text1, text2):
    """ä½¿ç”¨é›†åˆç›¸ä¼¼åº¦è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
    if not text1 or not text2:
        return 0.0
    set1 = set(text1)
    set2 = set(text2)
    common = set1.intersection(set2)
    if not set2:
        return 0.0
    return len(common) / len(set2)

def merge_stream_text(current_text, new_text):
    if not new_text:
        return current_text
    if not current_text:
        return new_text
    if new_text in current_text:
        return current_text

    max_overlap = 0
    max_len = min(len(current_text), len(new_text))
    for i in range(1, max_len + 1):
        if current_text[-i:] == new_text[:i]:
            max_overlap = i
    return current_text + new_text[max_overlap:]

def stream_recognition_from_samples(sample_iter, label=""):
    global all_audio_data

    cache = {}
    full_text = ""
    audio_cache = []
    chunk_size = [0, 10, 5]
    chunk_stride_samples = int(chunk_size[1] * 960)  # 600ms
    max_buffer = max(WINDOW_SIZE, chunk_stride_samples)

    if label:
        print(label)

    for audio_chunk in sample_iter:
        if not is_running:
            break

        # ä¿å­˜åˆ°å…¨å±€éŸ³é¢‘æ•°æ®ä¸­
        all_audio_data.extend(audio_chunk)

        # æ·»åŠ åˆ°æ»‘åŠ¨çª—å£ç¼“å­˜
        audio_cache.extend(audio_chunk)
        if len(audio_cache) > max_buffer:
            audio_cache = audio_cache[-max_buffer:]

        # è®¡ç®—å½“å‰éŸ³é‡
        current_volume = np.abs(audio_chunk).mean()

        # éŸ³é‡çŠ¶æ€æŒ‡ç¤º
        if current_volume < volume_threshold:
            volume_status = "ğŸ”‡ é™éŸ³"
        elif current_volume < volume_threshold * 2:
            volume_status = "ğŸ”‰ å°å£°"
        elif current_volume < volume_threshold * 5:
            volume_status = "ğŸ”Š æ­£å¸¸"
        else:
            volume_status = "ğŸ”ŠğŸ”Š å¤§å£°"

        # æŒ‰ 600ms æ­¥é•¿ç´¯ç§¯åé€å…¥æ¨¡å‹ï¼Œé¿å…è¿‡çŸ­å—å¯¼è‡´è¾“å‡ºå¡åœ¨â€œå—¯â€
        if len(audio_cache) < chunk_stride_samples:
            debug_info = (
                f"\r{volume_status} | éŸ³é‡: {current_volume:5.1f} | "
                f"ç¼“å­˜: {len(audio_cache):4d} | è¯†åˆ«ç»“æœ: {full_text}"
            )
            print(debug_info, end="", flush=True)
            continue

        processed_audio = preprocess_audio(np.array(audio_cache[:chunk_stride_samples]), gain=gain)
        audio_cache = audio_cache[chunk_stride_samples:]

        recognize_start = time.time()
        res = model.generate(
            input=processed_audio,
            cache=cache,
            is_final=False,
            chunk_size=chunk_size,
            encoder_chunk_look_back=2,
            decoder_chunk_look_back=1,
            disable_pbar=True,
            disable_log=True
        )
        recognize_delay = (time.time() - recognize_start) * 1000

        partial_text = res[0]["text"] if (res and res[0]["text"]) else ""
        if partial_text:
            full_text = merge_stream_text(full_text, partial_text)

        debug_info = (
            f"\r{volume_status} | éŸ³é‡: {current_volume:5.1f} | "
            f"å»¶è¿Ÿ: {recognize_delay:4.1f}ms | è¯†åˆ«ç»“æœ: {full_text}"
        )
        print(debug_info, end="", flush=True)

    # flush remaining cache
    if audio_cache:
        processed_audio = preprocess_audio(np.array(audio_cache), gain=gain)
        res = model.generate(
            input=processed_audio,
            cache=cache,
            is_final=True,
            chunk_size=chunk_size,
            encoder_chunk_look_back=2,
            decoder_chunk_look_back=1,
            disable_pbar=True,
            disable_log=True
        )
    else:
        res = model.generate(
            input=np.array([], dtype=np.int16),
            cache=cache,
            is_final=True,
            chunk_size=chunk_size,
            encoder_chunk_look_back=2,
            decoder_chunk_look_back=1,
            disable_pbar=True,
            disable_log=True
        )

    if res and res[0]["text"]:
        full_text = merge_stream_text(full_text, res[0]["text"])
        print(f"\nğŸ“ æœ€ç»ˆæµå¼è¯†åˆ«ç»“æœ: {full_text}")

    return full_text

# éº¦å…‹é£å®æ—¶å½•éŸ³å’Œè¯†åˆ«
def real_time_recognition():
    global is_running, all_audio_data

    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=SAMPLE_RATE,
                    input=True,
                    frames_per_buffer=CHUNK)

    print("ğŸ¤ å¼€å§‹å®æ—¶å½•éŸ³å’Œè¯†åˆ«...")
    print("   æŒ‰ Ctrl+C åœæ­¢\n")

    def mic_iter():
        while is_running:
            data = stream.read(CHUNK, exception_on_overflow=False)
            yield np.frombuffer(data, dtype=np.int16)

    try:
        result = stream_recognition_from_samples(mic_iter())
    except Exception as e:
        print(f"\nå½•éŸ³å‡ºé”™: {e}")
        result = ""
    finally:
        # æ¸…ç†èµ„æº
        stream.stop_stream()
        stream.close()
        p.terminate()

        # ä¿å­˜å½•éŸ³
        if all_audio_data:
            save_recording(np.array(all_audio_data))

        # ä½¿ç”¨å®Œæ•´éŸ³é¢‘è¿›è¡Œæœ€ç»ˆè¯†åˆ«
        if all_audio_data:
            print("\n\nğŸ” ä½¿ç”¨å®Œæ•´å½•éŸ³è¿›è¡Œæœ€ç»ˆè¯†åˆ«...")
            full_audio = np.array(all_audio_data)
            processed_full = preprocess_audio(full_audio)

            full_cache = {}
            res = model.generate(
                input=processed_full,
                cache=full_cache,
                is_final=True,
                chunk_size=[0, 10, 5],
                encoder_chunk_look_back=2,
                decoder_chunk_look_back=1,
                disable_pbar=True,
                disable_log=True
            )

            if res and res[0]['text']:
                final_text = res[0]['text']
                print(f"ğŸ“ å®Œæ•´å½•éŸ³è¯†åˆ«ç»“æœ: {final_text}")
                if result:
                    print(f"ğŸ”„ å®æ—¶è¯†åˆ«ç»“æœ: {result}")
                return final_text

    return result

# éŸ³é¢‘æ–‡ä»¶æµå¼é‡æ”¾
def file_streaming_recognition(audio_file):
    global all_audio_data
    
    print(f"ğŸ“ ä½¿ç”¨å½•éŸ³æ–‡ä»¶è¿›è¡Œæµ‹è¯•: {audio_file}")
    audio_data = np.load(audio_file)
    all_audio_data = audio_data.tolist()
    
    print(f"éŸ³é¢‘æ—¶é•¿: {len(audio_data)/16000:.2f}ç§’")
    print(f"å¹³å‡éŸ³é‡: {np.abs(audio_data).mean():.2f}")
    
    # 1. æŒ‰å®æ—¶æµå¼æ–¹å¼åˆ†å—é€å…¥æ¨¡å‹ï¼Œå¤ç°å®æ—¶è¡Œä¸º
    def file_iter():
        for i in range(0, len(audio_data), CHUNK):
            yield audio_data[i:i + CHUNK]

    final_text = stream_recognition_from_samples(file_iter(), label="\nå¼€å§‹æµå¼è¯†åˆ«...")
    print(f"\næœ€ç»ˆè¯†åˆ«ç»“æœ: {final_text}")
    return final_text

# ä¸»æµç¨‹
def main():
    global is_running
    
    # æ³¨å†Œä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    
    result = ""
    
    if args.audio_file:
        # ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶æµ‹è¯•
        result = file_streaming_recognition(args.audio_file)
    elif args.mic:
        # ä½¿ç”¨éº¦å…‹é£å®æ—¶è¾“å…¥
        result = real_time_recognition()
    else:
        print("è¯·æŒ‡å®š --mic ä½¿ç”¨éº¦å…‹é£ï¼Œæˆ– --audio_file æŒ‡å®šéŸ³é¢‘æ–‡ä»¶")
        return
    
    # è‡ªåŠ¨éªŒè¯
    if args.benchmark and result:
        print(f"\nâœ… è‡ªåŠ¨éªŒè¯:")
        print(f"   åŸºå‡†æ–‡æœ¬: {args.benchmark}")
        print(f"   è¯†åˆ«ç»“æœ: {result}")
        similarity = calculate_similarity(result, args.benchmark)
        print(f"   ç›¸ä¼¼åº¦: {similarity:.2f}")
        if similarity >= 0.7:
            print(f"   éªŒè¯çŠ¶æ€: é€šè¿‡ âœ…")
        else:
            print(f"   éªŒè¯çŠ¶æ€: æœªé€šè¿‡ âŒ")

if __name__ == "__main__":
    main()
    print("\n=== ç¨‹åºç»“æŸ ===")

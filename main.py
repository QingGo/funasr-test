import pyaudio
import numpy as np
from funasr import AutoModel
import logging
import time
import argparse
import signal
import os

# å±è”½ç¹æ‚æ—¥å¿—
logging.getLogger("modelscope").setLevel(logging.ERROR)
logging.getLogger("funasr").setLevel(logging.ERROR)

# å…¨å±€å˜é‡
is_running = True
cleanup_called = False
all_audio_data = []

# ä¿¡å·å¤„ç†
def signal_handler(sig, frame):
    global is_running
    print("\næ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
    is_running = False

# å‘½ä»¤è¡Œå‚æ•°è§£æ
parser = argparse.ArgumentParser(description="FunASR å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ")
parser.add_argument("--audio_file", type=str, help="å½•éŸ³æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæµå¼é‡æ”¾æµ‹è¯•")
parser.add_argument("--benchmark", type=str, help="åŸºå‡†æ–‡æœ¬ï¼Œç”¨äºè‡ªåŠ¨éªŒè¯")
parser.add_argument("--mic", action="store_true", help="ä½¿ç”¨éº¦å…‹é£å®æ—¶è¾“å…¥")
parser.add_argument("--gain", type=float, default=3.0, help="éŸ³é¢‘å¢ç›Šè°ƒæ•´ï¼Œé»˜è®¤3.0")
parser.add_argument("--threshold", type=float, default=100.0, help="éŸ³é‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è§†ä¸ºé™éŸ³ï¼Œé»˜è®¤100.0")
args = parser.parse_args()

print("=== FunASR å®æ—¶è¯­éŸ³è¯†åˆ«ç³»ç»Ÿ ===")
print("æ”¯æŒå®æ—¶éº¦å…‹é£è¾“å…¥ã€æ»‘åŠ¨çª—å£ã€å½•éŸ³ä¿å­˜å’Œè‡ªåŠ¨éªŒè¯åŠŸèƒ½\n")

# éŸ³é¢‘é…ç½®
SAMPLE_RATE = 16000
CHUNK = 960  # 60msï¼Œæ¯æ¬¡è¯»å–çš„éŸ³é¢‘å—å¤§å°
WINDOW_SIZE = 4800  # 300msï¼Œæ»‘åŠ¨çª—å£å¤§å°
STEP_SIZE = 960     # 60msï¼Œæ»‘åŠ¨æ­¥é•¿
FORMAT = pyaudio.paInt16
CHANNELS = 1
gain = args.gain
volume_threshold = args.threshold

print(f"ğŸ›ï¸  é…ç½®å‚æ•°:")
print(f"   CHUNK: {CHUNK} ({CHUNK/SAMPLE_RATE*1000:.0f}ms)")
print(f"   æ»‘åŠ¨çª—å£: {WINDOW_SIZE} ({WINDOW_SIZE/SAMPLE_RATE*1000:.0f}ms)")
print(f"   æ»‘åŠ¨æ­¥é•¿: {STEP_SIZE} ({STEP_SIZE/SAMPLE_RATE*1000:.0f}ms)")
print(f"   é‡‡æ ·ç‡: {SAMPLE_RATE}Hz")
print(f"   éŸ³é¢‘å¢ç›Š: {gain}")
print(f"   éŸ³é‡é˜ˆå€¼: {volume_threshold}")
print()

# åŠ è½½æ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = AutoModel(
    model="paraformer-zh-streaming", 
    model_revision="v2.0.4",
    disable_update=True,
    verbose=False
)
print("æ¨¡å‹åŠ è½½å®Œæˆï¼\n")

# éŸ³é¢‘é¢„å¤„ç†
def preprocess_audio(audio_chunk, gain=3.0):
    processed = audio_chunk.astype(np.float32) * gain
    processed = np.clip(processed, -32768, 32767)
    return processed.astype(np.int16)

# ä¿å­˜å½•éŸ³
def save_recording(audio_data):
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"recording_{timestamp}.npy"
    np.save(filename, audio_data)
    print(f"\nå½•éŸ³å·²ä¿å­˜ä¸º: {filename}")
    return filename

# ç›¸ä¼¼åº¦è®¡ç®—
def calculate_similarity(text1, text2):
    """ä½¿ç”¨é›†åˆç›¸ä¼¼åº¦è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
    if not text1 or not text2:
        return 0.0
    set1 = set(text1)
    set2 = set(text2)
    common = set1.intersection(set2)
    if not set2:
        return 0.0
    return len(common) / len(set2)

# éº¦å…‹é£å®æ—¶å½•éŸ³å’Œè¯†åˆ«
def real_time_recognition():
    global is_running, all_audio_data
    
    p = pyaudio.PyAudio()
    stream = p.open(format=FORMAT,
                    channels=CHANNELS,
                    rate=SAMPLE_RATE,
                    input=True,
                    frames_per_buffer=CHUNK)
    
    print("ğŸ¤ å¼€å§‹å®æ—¶å½•éŸ³å’Œè¯†åˆ«...")
    print("   æŒ‰ Ctrl+C åœæ­¢\n")
    
    # åˆå§‹åŒ–éŸ³é¢‘ç¼“å­˜
    audio_cache = []
    cache = {}
    last_text = ""
    start_time = time.time()
    
    try:
        while is_running:
            # è¯»å–éŸ³é¢‘æ•°æ®
            data = stream.read(CHUNK, exception_on_overflow=False)
            audio_chunk = np.frombuffer(data, dtype=np.int16)
            
            # ä¿å­˜åˆ°å…¨å±€éŸ³é¢‘æ•°æ®ä¸­
            all_audio_data.extend(audio_chunk)
            
            # æ·»åŠ åˆ°æ»‘åŠ¨çª—å£ç¼“å­˜
            audio_cache.extend(audio_chunk)
            
            # ä¿æŒç¼“å­˜å¤§å°
            if len(audio_cache) > WINDOW_SIZE:
                audio_cache = audio_cache[-WINDOW_SIZE:]
            
            # è®¡ç®—å½“å‰éŸ³é‡
            current_volume = np.abs(audio_chunk).mean()
            
            # éŸ³é‡çŠ¶æ€æŒ‡ç¤º
            if current_volume < volume_threshold:
                volume_status = "ğŸ”‡ é™éŸ³"
            elif current_volume < volume_threshold * 2:
                volume_status = "ğŸ”‰ å°å£°"
            elif current_volume < volume_threshold * 5:
                volume_status = "ğŸ”Š æ­£å¸¸"
            else:
                volume_status = "ğŸ”ŠğŸ”Š å¤§å£°"
            
            # æ›´é¢‘ç¹çš„è¯†åˆ«ï¼šå½“ç¼“å­˜è¾¾åˆ°STEP_SIZEå°±è¿›è¡Œè¯†åˆ«
            if len(audio_cache) >= STEP_SIZE:
                processed_audio = preprocess_audio(np.array(audio_cache), gain=gain)
                
                # è®°å½•è¯†åˆ«å¼€å§‹æ—¶é—´
                recognize_start = time.time()
                
                # æµå¼è¯†åˆ«
                res = model.generate(
                    input=processed_audio,
                    cache=cache,
                    is_final=False,
                    chunk_size=[0, 10, 5],
                    encoder_chunk_look_back=2,
                    decoder_chunk_look_back=1,
                    disable_pbar=True,
                    disable_log=True
                )
                
                # è®¡ç®—è¯†åˆ«å»¶è¿Ÿ
                recognize_delay = (time.time() - recognize_start) * 1000
                
                if res and res[0]['text']:
                    last_text = res[0]['text']
                    # æ˜¾ç¤ºæ›´å¤šè°ƒè¯•ä¿¡æ¯
                    debug_info = f"\r{volume_status} | éŸ³é‡: {current_volume:5.1f} | ç¼“å­˜: {len(audio_cache):4d} | å»¶è¿Ÿ: {recognize_delay:4.1f}ms | è¯†åˆ«ç»“æœ: {last_text}"
                    print(debug_info, end="", flush=True)
                    
                    # è‡ªåŠ¨éªŒè¯
                    if args.benchmark:
                        similarity = calculate_similarity(last_text, args.benchmark)
                        if similarity >= 0.7:
                            print(f"\nâœ… è¯†åˆ«ç»“æœä¸åŸºå‡†æ–‡æœ¬åŒ¹é…ï¼Œç›¸ä¼¼åº¦: {similarity:.2f}")
                            is_running = False
                else:
                    # å³ä½¿æ²¡æœ‰è¯†åˆ«ç»“æœï¼Œä¹Ÿæ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                    debug_info = f"\r{volume_status} | éŸ³é‡: {current_volume:5.1f} | ç¼“å­˜: {len(audio_cache):4d} | å»¶è¿Ÿ: {recognize_delay:4.1f}ms | è¯†åˆ«ç»“æœ: {last_text}"
                    print(debug_info, end="", flush=True)
    
    except Exception as e:
        print(f"\nå½•éŸ³å‡ºé”™: {e}")
    
    finally:
        # æ¸…ç†èµ„æº
        stream.stop_stream()
        stream.close()
        p.terminate()
        
        # ä¿å­˜å½•éŸ³
        recorded_file = None
        if all_audio_data:
            recorded_file = save_recording(np.array(all_audio_data))
        
        # ä½¿ç”¨å®Œæ•´éŸ³é¢‘è¿›è¡Œæœ€ç»ˆè¯†åˆ«
        if all_audio_data:
            print("\n\nğŸ” ä½¿ç”¨å®Œæ•´å½•éŸ³è¿›è¡Œæœ€ç»ˆè¯†åˆ«...")
            full_audio = np.array(all_audio_data)
            processed_full = preprocess_audio(full_audio)
            
            # é‡ç½®ç¼“å­˜ï¼Œä½¿ç”¨å®Œæ•´éŸ³é¢‘è¿›è¡Œè¯†åˆ«
            full_cache = {}
            res = model.generate(
                input=processed_full,
                cache=full_cache,
                is_final=True,
                chunk_size=[0, 10, 5],
                encoder_chunk_look_back=2,
                decoder_chunk_look_back=1,
                disable_pbar=True,
                disable_log=True
            )
            
            if res and res[0]['text']:
                final_text = res[0]['text']
                print(f"ğŸ“ å®Œæ•´å½•éŸ³è¯†åˆ«ç»“æœ: {final_text}")
                if last_text:
                    print(f"ğŸ”„ å®æ—¶è¯†åˆ«ç»“æœ: {last_text}")
                return final_text
        
        # å¦‚æœæ²¡æœ‰å®Œæ•´éŸ³é¢‘ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªç¼“å­˜
        elif audio_cache:
            final_processed = preprocess_audio(np.array(audio_cache))
            res = model.generate(
                input=final_processed,
                cache=cache,
                is_final=True,
                chunk_size=[0, 10, 5],
                encoder_chunk_look_back=2,
                decoder_chunk_look_back=1,
                disable_pbar=True,
                disable_log=True
            )
            
            if res and res[0]['text']:
                final_text = res[0]['text']
                print(f"\næœ€ç»ˆè¯†åˆ«ç»“æœ: {final_text}")
                return final_text
    
    return last_text

# éŸ³é¢‘æ–‡ä»¶æµå¼é‡æ”¾
def file_streaming_recognition(audio_file):
    global all_audio_data
    
    print(f"ğŸ“ ä½¿ç”¨å½•éŸ³æ–‡ä»¶è¿›è¡Œæµ‹è¯•: {audio_file}")
    audio_data = np.load(audio_file)
    all_audio_data = audio_data.copy()
    
    print(f"éŸ³é¢‘æ—¶é•¿: {len(audio_data)/16000:.2f}ç§’")
    print(f"å¹³å‡éŸ³é‡: {np.abs(audio_data).mean():.2f}")
    print("\nå¼€å§‹æµå¼è¯†åˆ«...")
    
    # 1. ç›´æ¥ä½¿ç”¨å®Œæ•´éŸ³é¢‘è¿›è¡Œæµå¼è¯†åˆ«ï¼ˆå·²çŸ¥å¯ä»¥å¾—åˆ°æ­£ç¡®ç»“æœï¼‰
    cache = {}
    processed_audio = preprocess_audio(audio_data, gain=gain)
    
    res = model.generate(
        input=processed_audio,
        cache=cache,
        is_final=False,
        chunk_size=[0, 10, 5],
        encoder_chunk_look_back=2,
        decoder_chunk_look_back=1,
        disable_pbar=True,
        disable_log=True
    )
    
    if res and res[0]['text']:
        print(f"\nä¸­é—´è¯†åˆ«ç»“æœ: {res[0]['text']}")
    
    res = model.generate(
        input=np.array([], dtype=np.int16),
        cache=cache,
        is_final=True,
        chunk_size=[0, 10, 5],
        encoder_chunk_look_back=2,
        decoder_chunk_look_back=1,
        disable_pbar=True,
        disable_log=True
    )
    
    final_text = res[0]['text'] if (res and res[0]['text']) else ""
    print(f"æœ€ç»ˆè¯†åˆ«ç»“æœ: {final_text}")
    
    return final_text

# ä¸»æµç¨‹
def main():
    global is_running
    
    # æ³¨å†Œä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    
    result = ""
    
    if args.audio_file:
        # ä½¿ç”¨éŸ³é¢‘æ–‡ä»¶æµ‹è¯•
        result = file_streaming_recognition(args.audio_file)
    elif args.mic:
        # ä½¿ç”¨éº¦å…‹é£å®æ—¶è¾“å…¥
        result = real_time_recognition()
    else:
        print("è¯·æŒ‡å®š --mic ä½¿ç”¨éº¦å…‹é£ï¼Œæˆ– --audio_file æŒ‡å®šéŸ³é¢‘æ–‡ä»¶")
        return
    
    # è‡ªåŠ¨éªŒè¯
    if args.benchmark and result:
        print(f"\nâœ… è‡ªåŠ¨éªŒè¯:")
        print(f"   åŸºå‡†æ–‡æœ¬: {args.benchmark}")
        print(f"   è¯†åˆ«ç»“æœ: {result}")
        similarity = calculate_similarity(result, args.benchmark)
        print(f"   ç›¸ä¼¼åº¦: {similarity:.2f}")
        if similarity >= 0.7:
            print(f"   éªŒè¯çŠ¶æ€: é€šè¿‡ âœ…")
        else:
            print(f"   éªŒè¯çŠ¶æ€: æœªé€šè¿‡ âŒ")

if __name__ == "__main__":
    main()
    print("\n=== ç¨‹åºç»“æŸ ===")